{"version":3,"file":"js/403.10b3efc3.js","mappings":"meASMA,EAAa,CAAEC,MAAO,gBACtBC,EAAa,CAAED,MAAO,iBACtBE,EAAa,CAAEF,MAAO,iBACtBG,EAAa,CAAEH,MAAO,uBACtBI,EAAa,CAAC,WACdC,EAAa,CAAEL,MAAO,cACtBM,EAAa,CAAEN,MAAO,YACtBO,EAAa,CAAEP,MAAO,gBACtBQ,EAAa,CAAC,OACdC,EAAc,CAAET,MAAO,0BACvBU,EAAc,CAClBV,MAAO,UACPW,GAAI,YAEAC,EAAc,CAAEZ,MAAO,mBACvBa,EAAc,CAClBb,MAAO,UACPW,GAAI,WAEAG,EAAc,CAAEd,MAAO,mBACvBe,EAAc,CAClBf,MAAO,UACPW,GAAI,kBAEAK,EAAc,CAAEhB,MAAO,mBACvBiB,EAAc,CAAEjB,MAAO,cACvBkB,EAAc,CAAElB,MAAO,cACvBmB,EAAc,CAAEnB,MAAO,cACvBoB,EAAc,CAClBpB,MAAO,UACPW,GAAI,eAEAU,EAAc,CAAErB,MAAO,mBAI7B,SAA4BsB,EAAAA,EAAAA,IAAiB,CAC3CC,OAAQ,cACRC,MAAK,SAACC,GCmRN,IAAMC,EAAY,CAChB,CACEC,MAAO,qBACPC,KAAM,cACNC,WAAY,QAEd,CACEF,MAAO,kBACPC,KAAM,cACNC,WAAY,QAEd,CACEF,MAAO,aACPC,KAAM,cACNC,WAAY,QAEd,CACEF,MAAO,oBACPC,KAAM,eACNC,WAAY,QAEd,CACEF,MAAO,eACPC,KAAM,eACNC,WAAY,QAEd,CACEF,MAAO,aACPC,KAAM,eACNC,WAAY,QAEd,CACEF,MAAO,aACPC,KAAM,cACNC,WAAY,SAIVC,EAAe,CACnB,CAAEH,MAAO,SAAUC,KAAM,cAAeC,WAAY,QACpD,CAAEF,MAAO,8BAA+BC,KAAM,cAAeC,WAAY,OACzE,CAAEF,MAAO,eAAgBC,KAAM,cAAeC,WAAY,KAC1D,CAAEF,MAAO,gBAAiBC,KAAM,eAAgBC,WAAY,QAC5D,CAAEF,MAAO,qCAAsCC,KAAM,cAAeC,WAAY,QAChF,CAAEF,MAAO,sBAAuBC,KAAM,cAAeC,WAAY,KACjE,CAAEF,MAAO,eAAgBC,KAAM,eAAgBC,WAAY,QAC3D,CAAEF,MAAO,oCAAqCC,KAAM,eAAgBC,WAAY,QAChF,CAAEF,MAAO,qBAAsBC,KAAM,cAAeC,WAAY,MAG5DE,EAAkB,SAACC,GACvB,IAAMC,EAAUC,SAASC,eAAeH,GACpCC,GACFA,EAAQG,eAAe,CAAEC,SAAU,UAEvC,EAGMC,EAAU,IAAIC,IAAI,cAAqCC,KACvDC,EAAc,IAAIF,IAAI,cAA0CC,KAChEE,EAAU,IAAIH,IAAI,cAAqCC,KACvDG,EAAc,IAAIJ,IAAI,cAA0CC,KAEhEI,EAAe,CACnB,CACEC,MAAO,KACPC,MAAO,+BACPnB,MAAO,0BACPoB,SAAU,6CACVC,IAAKV,GAEP,CACEO,MAAO,UACPC,MAAO,8BACPnB,MAAO,+BACPoB,SAAU,iCACVC,IAAKP,GAEP,CACEI,MAAO,KACPC,MAAO,+BACPnB,MAAO,0BACPoB,SAAU,6CACVC,IAAKN,GAEP,CACEG,MAAO,UACPC,MAAO,yBACPnB,MAAO,+BACPoB,SAAU,8BACVC,IAAKL,IAIHM,GAAgBC,EAAAA,EAAAA,IAAI,MAEpBC,GAAkBC,EAAAA,EAAAA,KAAS,WAC/B,IAAMC,EAAST,EAAaU,MAAK,SAAAC,GAAG,OAAIA,EAAIV,QAAUI,EAAcJ,KAAK,IACzE,OAAOQ,EAASA,EAAOL,IAAM,EAC/B,ID7QF,OAAO,SAACQ,EAAUC,GAChB,IAAMC,GAA0BC,EAAAA,EAAAA,IAAkB,gBAC5CC,GAAqBD,EAAAA,EAAAA,IAAkB,WACvCE,GAAoBF,EAAAA,EAAAA,IAAkB,UACtCG,GAAoBH,EAAAA,EAAAA,IAAkB,UACtCI,GAA6BJ,EAAAA,EAAAA,IAAkB,mBAC/CK,GAAsBL,EAAAA,EAAAA,IAAkB,YAE9C,OAAQM,EAAAA,EAAAA,OAAcC,EAAAA,EAAAA,IAAoB,MAAOnE,EAAY,EAC3DoE,EAAAA,EAAAA,IAAaP,EAAoB,CAC/BQ,KAAM,aACN,mBAAoB,mBACpB,aAAc,OACd,oBAAqB,QACpB,CACDC,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaT,EAAyB,CAAEa,MAAO,KAAO,CACpDF,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,KAAOA,EAAO,GAAK,EAChDe,EAAAA,EAAAA,IAAoB,OAAQ,CAAEC,MAAO,CAAC,cAAc,QAAU,SAAU,IACxE,IACFC,EAAG,KAELP,EAAAA,EAAAA,IAAaT,EAAyB,CACpCiB,QAASlB,EAAO,KAAOA,EAAO,GAAK,SAACmB,GAAW,OAAM7C,EAAgB,WAAW,IAC/E,CACDsC,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,KAAOA,EAAO,GAAK,EAChDoB,EAAAA,EAAAA,IAAiB,aACjB,IACFH,EAAG,KAELP,EAAAA,EAAAA,IAAaT,EAAyB,CACpCiB,QAASlB,EAAO,KAAOA,EAAO,GAAK,SAACmB,GAAW,OAAM7C,EAAgB,UAAU,IAC9E,CACDsC,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,KAAOA,EAAO,GAAK,EAChDoB,EAAAA,EAAAA,IAAiB,yCACjB,IACFH,EAAG,KAELP,EAAAA,EAAAA,IAAaT,EAAyB,CACpCiB,QAASlB,EAAO,KAAOA,EAAO,GAAK,SAACmB,GAAW,OAAM7C,EAAgB,iBAAiB,IACrF,CACDsC,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,KAAOA,EAAO,GAAK,EAChDoB,EAAAA,EAAAA,IAAiB,mBACjB,IACFH,EAAG,KAELP,EAAAA,EAAAA,IAAaT,EAAyB,CACpCiB,QAASlB,EAAO,KAAOA,EAAO,GAAK,SAACmB,GAAW,OAAM7C,EAAgB,cAAc,IAClF,CACDsC,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,KAAOA,EAAO,GAAK,EAChDoB,EAAAA,EAAAA,IAAiB,gBACjB,IACFH,EAAG,KAELP,EAAAA,EAAAA,IAAaT,EAAyB,CACpCiB,QAASlB,EAAO,KAAOA,EAAO,GAAK,SAACmB,GAAW,OAAM7C,EAAgB,YAAY,IAChF,CACDsC,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDoB,EAAAA,EAAAA,IAAiB,cACjB,IACFH,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMqB,EAAAA,EAAAA,IAAmB,w+CAA+gD,KAC9jDN,EAAAA,EAAAA,IAAoB,MAAOvE,EAAY,EACrCkE,EAAAA,EAAAA,IAAaL,EAAmB,CAAEiB,OAAQ,IAAM,CAC9CV,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,GAAK,CAC3CX,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBE,EAAAA,EAAAA,IAAoB,MAAOtE,EAAY,EACrCsE,EAAAA,EAAAA,IAAoB,MAAOrE,EAAY,CACrCsD,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,uBAAyB,8BAA+B,MACvHiE,EAAAA,EAAAA,OAAcC,EAAAA,EAAAA,IAAoBe,EAAAA,GAAW,MAAMC,EAAAA,EAAAA,IAAYtC,GAAc,SAACS,GAC7E,OAAOmB,EAAAA,EAAAA,IAAoB,MAAO,CAChCW,IAAK9B,EAAOR,MACZ7C,OAAOoF,EAAAA,EAAAA,IAAgB,CAAC,oBAAqB,CAAEC,OAAQpC,EAAcJ,QAAUQ,EAAOR,SACtF8B,QAAS,SAACC,GAAW,OAAM3B,EAAcJ,MAAQQ,EAAOR,KAAK,GAC5D,EACD2B,EAAAA,EAAAA,IAAoB,MAAOnE,GAAYiF,EAAAA,EAAAA,IAAiBjC,EAAO1B,OAAQ,IACvE6C,EAAAA,EAAAA,IAAoB,MAAOlE,GAAYgF,EAAAA,EAAAA,IAAiBjC,EAAON,UAAW,IACzE,GAAI3C,EACT,IAAI,SAGT,IACDsE,EAAG,KAELP,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBE,EAAAA,EAAAA,IAAoB,MAAOjE,EAAY,CACpC4C,EAAgBN,QACZoB,EAAAA,EAAAA,OAAcC,EAAAA,EAAAA,IAAoB,QAAS,CAC1CiB,IAAK,EACLnC,IAAKG,EAAgBN,MACrB0C,SAAU,GACVvF,MAAO,cACN,iDAAkD,EAAGQ,KACxDgF,EAAAA,EAAAA,IAAoB,IAAI,KAE/B,IACDd,EAAG,IAEN,IACDA,EAAG,OAGPF,EAAAA,EAAAA,IAAoB,MAAO/D,EAAa,EACtC0D,EAAAA,EAAAA,IAAaL,EAAmB,CAAE2B,QAAS,UAAY,CACrDpB,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBE,EAAAA,EAAAA,IAAoB,MAAO9D,EAAa,CACtC+C,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC1DA,EAAAA,EAAAA,IAAoB,OAAQ,CAAExE,MAAO,iBAAmB,cACtD,KACJwE,EAAAA,EAAAA,IAAoB,MAAO5D,EAAa,EACtCuD,EAAAA,EAAAA,IAAaL,EAAmB,CAAEiB,OAAQ,IAAM,CAC9CV,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,KACxCb,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,IAAK,KAAM,g+BAAi+B,IAChgC,IACFE,EAAG,KAELP,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,MAAO,CACzBxB,IAAK0C,EACLC,IAAK,WACL3F,MAAO,mCACN,MAAO,IACV,IACF0E,EAAG,IAEN,IACDA,EAAG,SAITF,EAAAA,EAAAA,IAAoB,MAAO3D,EAAa,CACtC4C,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC1DA,EAAAA,EAAAA,IAAoB,OAAQ,CAAExE,MAAO,iBAAmB,0CACtD,KACJwE,EAAAA,EAAAA,IAAoB,MAAO1D,EAAa,CACtC2C,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,KAAM,yTAA0T,IACrXf,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,wBAAyB,IACrFf,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,KAAM,mGAAoG,KAC/JL,EAAAA,EAAAA,IAAaH,EAAqB,CAChC4B,KAAMlE,EACN+C,MAAO,CAAC,MAAQ,QAChBoB,OAAQ,IACP,CACDxB,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,QACNhD,MAAO,QACPiD,MAAO,YAET5B,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,OACNhD,MAAO,mBACPiD,MAAO,YAET5B,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,aACNhD,MAAO,UACPiD,MAAO,UACN,CACD1B,SAASC,EAAAA,EAAAA,KAAS,SAAC0B,GAAK,MAAK,EAC3BnB,EAAAA,EAAAA,KAAiBS,EAAAA,EAAAA,IAAiBU,EAAMC,IAAIpE,YAAa,GAC1D,IACD6C,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,CAAExE,MAAO,iBAAmB,+EAAgF,IACjKyD,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,mBAAqB,EACnFwE,EAAAA,EAAAA,IAAoB,SAAU,KAAM,mBACpCK,EAAAA,EAAAA,IAAiB,4JACf,SAGRL,EAAAA,EAAAA,IAAoB,MAAOzD,EAAa,CACtC0C,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC1DA,EAAAA,EAAAA,IAAoB,OAAQ,CAAExE,MAAO,iBAAmB,oBACtD,KACJwE,EAAAA,EAAAA,IAAoB,MAAOxD,EAAa,CACtCyC,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,KAAM,2HAA4H,KACvLA,EAAAA,EAAAA,IAAoB,MAAOvD,EAAa,EACtCkD,EAAAA,EAAAA,IAAaL,EAAmB,CAAEiB,OAAQ,IAAM,CAC9CV,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,+BAAgC,IAChEA,EAAAA,EAAAA,IAAoB,IAAK,KAAM,iJAAkJ,IACjLA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,0EAChCA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,sHAC9B,IACJ,IACFE,EAAG,KAELP,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,MAAO,CACzBxB,IAAKkD,EACLP,IAAK,+BACL3F,MAAO,gBACN,MAAO,IACV,IACF0E,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,mBAAqB,EACnFwE,EAAAA,EAAAA,IAAoB,SAAU,KAAM,mBACpCK,EAAAA,EAAAA,IAAiB,mNACf,OAENL,EAAAA,EAAAA,IAAoB,MAAOtD,EAAa,EACtCiD,EAAAA,EAAAA,IAAaL,EAAmB,CAAEiB,OAAQ,IAAM,CAC9CV,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,8CAA+C,IAC/EA,EAAAA,EAAAA,IAAoB,IAAK,KAAM,wJAAyJ,IACxLA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,gGAChCA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,uFAC9B,IACJ,IACFE,EAAG,KAELP,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,KACxCb,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,MAAO,CACzBxB,IAAKmD,EACLR,IAAK,6BACL3F,MAAO,gBACN,MAAO,IACV,IACF0E,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,mBAAqB,EACnFwE,EAAAA,EAAAA,IAAoB,SAAU,KAAM,mBACpCK,EAAAA,EAAAA,IAAiB,oHACf,OAENL,EAAAA,EAAAA,IAAoB,MAAOrD,EAAa,EACtCgD,EAAAA,EAAAA,IAAaL,EAAmB,CAAEiB,OAAQ,IAAM,CAC9CV,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,yCAA0C,IAC1EA,EAAAA,EAAAA,IAAoB,IAAK,KAAM,kHAAmH,IAClJA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,SAAU,KAAM,kBACpCK,EAAAA,EAAAA,IAAiB,2FAEnBL,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,SAAU,KAAM,wBACpCK,EAAAA,EAAAA,IAAiB,oEAEnBL,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,SAAU,KAAM,wBACpCK,EAAAA,EAAAA,IAAiB,2DAEjB,IACJ,IACFH,EAAG,KAELP,EAAAA,EAAAA,IAAaN,EAAmB,CAAEmB,KAAM,IAAM,CAC5CX,SAASC,EAAAA,EAAAA,KAAS,kBAAMb,EAAO,MAAQA,EAAO,IAAM,EAClDe,EAAAA,EAAAA,IAAoB,MAAO,CACzBxB,IAAKoD,EACLT,IAAK,yBACL3F,MAAO,gBACN,MAAO,IACV,IACF0E,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,mBAAqB,EACnFwE,EAAAA,EAAAA,IAAoB,SAAU,KAAM,mBACpCK,EAAAA,EAAAA,IAAiB,gXACf,WAIVL,EAAAA,EAAAA,IAAoB,MAAOpD,EAAa,CACtCqC,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC1DA,EAAAA,EAAAA,IAAoB,OAAQ,CAAExE,MAAO,iBAAmB,iBACtD,KACJwE,EAAAA,EAAAA,IAAoB,MAAOnD,EAAa,CACtCoC,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,KAAM,iMAAkM,IAC7Pf,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAEC,MAAO,CAAC,aAAa,WAAa,EACzFD,EAAAA,EAAAA,IAAoB,MAAO,CACzBxB,IAAKqD,EACLV,IAAK,qBACL3F,MAAO,mBACPyE,MAAO,CAAC,MAAQ,WAEhB,IACJhB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,iBAAmB,EACjFwE,EAAAA,EAAAA,IAAoB,KAAM,KAAM,0BAChCA,EAAAA,EAAAA,IAAoB,IAAK,KAAM,8NAC7B,IACJf,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,iBAAmB,EACjFwE,EAAAA,EAAAA,IAAoB,KAAM,KAAM,6CAChCA,EAAAA,EAAAA,IAAoB,IAAK,KAAM,kMAC7B,IACJf,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,eAAgB,IAC5Ef,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC1DA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,sFAChCA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,kDAChCA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,2EAC9B,KACJL,EAAAA,EAAAA,IAAaH,EAAqB,CAChC4B,KAAM9D,EACN2C,MAAO,CAAC,MAAQ,QAChBoB,OAAQ,IACP,CACDxB,SAASC,EAAAA,EAAAA,KAAS,iBAAM,EACtBH,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,QACNhD,MAAO,QACPiD,MAAO,YAET5B,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,OACNhD,MAAO,mBACPiD,MAAO,YAET5B,EAAAA,EAAAA,IAAaJ,EAA4B,CACvC+B,KAAM,aACNhD,MAAO,UACPiD,MAAO,UACN,CACD1B,SAASC,EAAAA,EAAAA,KAAS,SAAC0B,GAAK,MAAK,EAC3BnB,EAAAA,EAAAA,KAAiBS,EAAAA,EAAAA,IAAiBU,EAAMC,IAAIpE,YAAa,GAC1D,IACD6C,EAAG,IAEN,IACDA,EAAG,IAELjB,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,IAAK,CAAExE,MAAO,iBAAmB,mFAAoF,QAGzKyD,EAAO,MAAQA,EAAO,KAAMe,EAAAA,EAAAA,IAAoB,MAAO,CACrDxE,MAAO,UACPW,GAAI,aACH,EACD6D,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,OAAQ,CAAExE,MAAO,iBAAmB,gBAE1DwE,EAAAA,EAAAA,IAAoB,MAAO,CAAExE,MAAO,mBAAqB,EACvDwE,EAAAA,EAAAA,IAAoB,IAAK,KAAM,8DAC/BA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,KAAM,KAAM,EAC9BA,EAAAA,EAAAA,IAAoB,IAAK,CACvBhC,KAAM,6CACN8D,OAAQ,UACP,wBAIP,IACL,IACD5B,EAAG,IAEN,IACDA,EAAG,OAIX,CACA,I,eEvhBA,MAAM6B,GAA2B,OAAgB,EAAQ,CAAC,CAAC,YAAY,qBAEvE,G","sources":["webpack://X-ISC/./src/views/ProjectView.vue?cc76","webpack://X-ISC/./src/views/ProjectView.vue","webpack://X-ISC/./src/views/ProjectView.vue?09a1"],"sourcesContent":["import { defineComponent as _defineComponent } from 'vue'\nimport { createElementVNode as _createElementVNode, resolveComponent as _resolveComponent, withCtx as _withCtx, createVNode as _createVNode, createTextVNode as _createTextVNode, openBlock as _openBlock, createElementBlock as _createElementBlock, renderList as _renderList, Fragment as _Fragment, toDisplayString as _toDisplayString, normalizeClass as _normalizeClass, createCommentVNode as _createCommentVNode, createStaticVNode as _createStaticVNode } from \"vue\"\nimport _imports_0 from '@/figures/overviewf3.png'\nimport _imports_1 from '@/figures/internal_confidence.png'\nimport _imports_2 from '@/figures/case.png'\nimport _imports_3 from '@/figures/humanCognitiveBiasf.png'\nimport _imports_4 from '@/figures/mergedMitigation2.png'\n\n\nconst _hoisted_1 = { class: \"project-page\" }\nconst _hoisted_2 = { class: \"video-section\" }\nconst _hoisted_3 = { class: \"video-options\" }\nconst _hoisted_4 = { class: \"video-selector-list\" }\nconst _hoisted_5 = [\"onClick\"]\nconst _hoisted_6 = { class: \"model-name\" }\nconst _hoisted_7 = { class: \"question\" }\nconst _hoisted_8 = { class: \"video-player\" }\nconst _hoisted_9 = [\"src\"]\nconst _hoisted_10 = { class: \"container main-content\" }\nconst _hoisted_11 = {\n  class: \"section\",\n  id: \"abstract\"\n}\nconst _hoisted_12 = { class: \"section-content\" }\nconst _hoisted_13 = {\n  class: \"section\",\n  id: \"failure\"\n}\nconst _hoisted_14 = { class: \"section-content\" }\nconst _hoisted_15 = {\n  class: \"section\",\n  id: \"interpretation\"\n}\nconst _hoisted_16 = { class: \"section-content\" }\nconst _hoisted_17 = { class: \"method-box\" }\nconst _hoisted_18 = { class: \"method-box\" }\nconst _hoisted_19 = { class: \"method-box\" }\nconst _hoisted_20 = {\n  class: \"section\",\n  id: \"alleviation\"\n}\nconst _hoisted_21 = { class: \"section-content\" }\n\nimport { ref, computed } from 'vue';\n  \nexport default /*@__PURE__*/_defineComponent({\n  __name: 'ProjectView',\n  setup(__props) {\n\n  // No additional scripts needed for static content\n  \n  // Sample Data for Tables\n  const boolqData = [\n    { \n      model: 'ChatGPT o1-preview', \n      acc1: '78.7 (↓4.9)', \n      overturned: '13.2' \n    },\n    { \n      model: 'ChatGPT o1-mini', \n      acc1: '74.1 (↓4.2)', \n      overturned: '15.6' \n    },\n    { \n      model: 'ChatGPT 4o', \n      acc1: '79.2 (↓4.9)', \n      overturned: '11.3' \n    },\n    { \n      model: 'ChatGPT 3.5-turbo', \n      acc1: '62.5 (↓12.1)', \n      overturned: '34.0' \n    },\n    { \n      model: 'Llama-3.1-8B', \n      acc1: '49.2 (↓20.4)', \n      overturned: '58.8' \n    },\n    { \n      model: 'Llama-3-8B', \n      acc1: '50.1 (↓20.3)', \n      overturned: '58.2' \n    },\n    { \n      model: 'Llama-2-7B', \n      acc1: '52.8 (↓8.7)', \n      overturned: '26.5' \n    }\n  ];\n  \n  const mitigateData = [\n    { model: 'GPT-4o', acc1: '79.2 (↓4.9)', overturned: '11.3' },\n    { model: 'GPT-4o + Question repeating', acc1: '83.6 (↓0.5)', overturned: '6.0' },\n    { model: 'GPT-4o + SFT', acc1: '87.7 (↑4.1)', overturned: '0' },\n    { model: 'GPT-3.5-turbo', acc1: '62.5 (↓12.1)', overturned: '34.0' },\n    { model: 'GPT-3.5-turbo + Question repeating', acc1: '67.4 (↓7.2)', overturned: '23.1' },\n    { model: 'GPT-3.5-turbo + SFT', acc1: '76.2 (↑1.6)', overturned: '0' },\n    { model: 'Llama-3.1-8B', acc1: '49.2 (↓20.4)', overturned: '58.8' },\n    { model: 'Llama-3.1-8B + Question repeating', acc1: '52.4 (↓17.2)', overturned: '52.8' },\n    { model: 'Llama-3.1-8B + SFT', acc1: '70.3 (↑0.7)', overturned: '0' }\n  ];\n  \n  const scrollToSection = (sectionId: string) => {\n    const element = document.getElementById(sectionId);\n    if (element) {\n      element.scrollIntoView({ behavior: 'smooth' });\n    }\n  };\n  \n  const video4o = new URL('@/figures/4o.mov', import.meta.url).href\n  const video4oMini = new URL('@/figures/4o-mini.mov', import.meta.url).href\n  const videoO1 = new URL('@/figures/o1.mov', import.meta.url).href\n  const videoO1Mini = new URL('@/figures/o1-mini.mov', import.meta.url).href\n  \n  const videoOptions = [\n    {\n      value: '4o',\n      label: 'GPT-4o - Population Question',\n      model: 'ChatGPT 4o (2024.12.17)',\n      question: 'Does China has more population than India?',\n      src: video4o\n    },\n    {\n      value: '4o-mini',\n      label: 'GPT-4o - Moon Jump Question',\n      model: 'ChatGPT 4o mini (2024.12.17)',\n      question: 'Can I jump from Earth to Moon?',\n      src: video4oMini\n    },\n    {\n      value: 'o1',\n      label: 'GPT-o1 - Population Question',\n      model: 'ChatGPT o1 (2024.12.17)',\n      question: 'Does China has more population than India?',\n      src: videoO1\n    },\n    {\n      value: 'o1-mini',\n      label: 'GPT-o1 - Arms Question',\n      model: 'ChatGPT o1-mini (2024.12.17)',\n      question: 'Does human have three arms?',\n      src: videoO1Mini\n    }\n  ];\n  \n  const selectedVideo = ref('4o');\n  \n  const currentVideoSrc = computed(() => {\n    const option = videoOptions.find(opt => opt.value === selectedVideo.value);\n    return option ? option.src : '';\n  });\n  \nreturn (_ctx: any,_cache: any) => {\n  const _component_el_menu_item = _resolveComponent(\"el-menu-item\")!\n  const _component_el_menu = _resolveComponent(\"el-menu\")!\n  const _component_el_col = _resolveComponent(\"el-col\")!\n  const _component_el_row = _resolveComponent(\"el-row\")!\n  const _component_el_table_column = _resolveComponent(\"el-table-column\")!\n  const _component_el_table = _resolveComponent(\"el-table\")!\n\n  return (_openBlock(), _createElementBlock(\"div\", _hoisted_1, [\n    _createVNode(_component_el_menu, {\n      mode: \"horizontal\",\n      \"background-color\": 'rgb(140, 21, 21)',\n      \"text-color\": \"#fff\",\n      \"active-text-color\": \"#fff\"\n    }, {\n      default: _withCtx(() => [\n        _createVNode(_component_el_menu_item, { index: \"/\" }, {\n          default: _withCtx(() => _cache[5] || (_cache[5] = [\n            _createElementVNode(\"span\", { style: {\"font-weight\":\"800\"} }, \"X-ISC\", -1)\n          ])),\n          _: 1\n        }),\n        _createVNode(_component_el_menu_item, {\n          onClick: _cache[0] || (_cache[0] = ($event: any) => (scrollToSection('abstract')))\n        }, {\n          default: _withCtx(() => _cache[6] || (_cache[6] = [\n            _createTextVNode(\"Abstract\")\n          ])),\n          _: 1\n        }),\n        _createVNode(_component_el_menu_item, {\n          onClick: _cache[1] || (_cache[1] = ($event: any) => (scrollToSection('failure')))\n        }, {\n          default: _withCtx(() => _cache[7] || (_cache[7] = [\n            _createTextVNode(\"Failure of Intrinsic Self-Correction\")\n          ])),\n          _: 1\n        }),\n        _createVNode(_component_el_menu_item, {\n          onClick: _cache[2] || (_cache[2] = ($event: any) => (scrollToSection('interpretation')))\n        }, {\n          default: _withCtx(() => _cache[8] || (_cache[8] = [\n            _createTextVNode(\"Interpretation\")\n          ])),\n          _: 1\n        }),\n        _createVNode(_component_el_menu_item, {\n          onClick: _cache[3] || (_cache[3] = ($event: any) => (scrollToSection('alleviation')))\n        }, {\n          default: _withCtx(() => _cache[9] || (_cache[9] = [\n            _createTextVNode(\"Alleviation\")\n          ])),\n          _: 1\n        }),\n        _createVNode(_component_el_menu_item, {\n          onClick: _cache[4] || (_cache[4] = ($event: any) => (scrollToSection('resources')))\n        }, {\n          default: _withCtx(() => _cache[10] || (_cache[10] = [\n            _createTextVNode(\"Resources\")\n          ])),\n          _: 1\n        })\n      ]),\n      _: 1\n    }),\n    _cache[41] || (_cache[41] = _createStaticVNode(\"<div class=\\\"container header\\\" data-v-3a126768><h2 class=\\\"title\\\" data-v-3a126768>Understanding the Dark Side of LLMs&#39; Intrinsic Self-Correction</h2><h4 class=\\\"subtitle\\\" data-v-3a126768><span class=\\\"underline\\\" data-v-3a126768>Ex</span>plaining <span class=\\\"underline\\\" data-v-3a126768>I</span>ntrinsic <span class=\\\"underline\\\" data-v-3a126768>S</span>elf-<span class=\\\"underline\\\" data-v-3a126768>C</span>orrection (X-ISC) </h4><div class=\\\"author-info\\\" data-v-3a126768><span data-v-3a126768>Anonymous submission</span></div><div class=\\\"github-link-container\\\" data-v-3a126768><a href=\\\"https://anonymous.4open.science/r/SC-15FB/\\\" class=\\\"github-button\\\" target=\\\"_blank\\\" data-v-3a126768><i class=\\\"github-icon\\\" data-v-3a126768><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\" fill=\\\"currentColor\\\" viewBox=\\\"0 0 16 16\\\" data-v-3a126768><path d=\\\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z\\\" data-v-3a126768></path></svg></i><span data-v-3a126768>Project Code</span></a></div></div>\", 1)),\n    _createElementVNode(\"div\", _hoisted_2, [\n      _createVNode(_component_el_row, { gutter: 10 }, {\n        default: _withCtx(() => [\n          _createVNode(_component_el_col, { span: 7 }, {\n            default: _withCtx(() => [\n              _createElementVNode(\"div\", _hoisted_3, [\n                _createElementVNode(\"div\", _hoisted_4, [\n                  _cache[11] || (_cache[11] = _createElementVNode(\"div\", { class: \"video-option-header\" }, \" Select Model & Question: \", -1)),\n                  (_openBlock(), _createElementBlock(_Fragment, null, _renderList(videoOptions, (option) => {\n                    return _createElementVNode(\"div\", {\n                      key: option.value,\n                      class: _normalizeClass([\"video-option-item\", { active: selectedVideo.value === option.value }]),\n                      onClick: ($event: any) => (selectedVideo.value = option.value)\n                    }, [\n                      _createElementVNode(\"div\", _hoisted_6, _toDisplayString(option.model), 1),\n                      _createElementVNode(\"div\", _hoisted_7, _toDisplayString(option.question), 1)\n                    ], 10, _hoisted_5)\n                  }), 64))\n                ])\n              ])\n            ]),\n            _: 1\n          }),\n          _createVNode(_component_el_col, { span: 15 }, {\n            default: _withCtx(() => [\n              _createElementVNode(\"div\", _hoisted_8, [\n                (currentVideoSrc.value)\n                  ? (_openBlock(), _createElementBlock(\"video\", {\n                      key: 0,\n                      src: currentVideoSrc.value,\n                      controls: \"\",\n                      class: \"demo-video\"\n                    }, \" Your browser does not support the video tag. \", 8, _hoisted_9))\n                  : _createCommentVNode(\"\", true)\n              ])\n            ]),\n            _: 1\n          })\n        ]),\n        _: 1\n      })\n    ]),\n    _createElementVNode(\"div\", _hoisted_10, [\n      _createVNode(_component_el_row, { justify: \"center\" }, {\n        default: _withCtx(() => [\n          _createVNode(_component_el_col, { span: 20 }, {\n            default: _withCtx(() => [\n              _createElementVNode(\"div\", _hoisted_11, [\n                _cache[14] || (_cache[14] = _createElementVNode(\"h3\", null, [\n                  _createElementVNode(\"span\", { class: \"section-title\" }, \"Abstract\")\n                ], -1)),\n                _createElementVNode(\"div\", _hoisted_12, [\n                  _createVNode(_component_el_row, { gutter: 20 }, {\n                    default: _withCtx(() => [\n                      _createVNode(_component_el_col, { span: 1 }),\n                      _createVNode(_component_el_col, { span: 11 }, {\n                        default: _withCtx(() => _cache[12] || (_cache[12] = [\n                          _createElementVNode(\"p\", null, \" Intrinsic self-correction was proposed to improve LLMs' responses via feedback solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases? By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design interpretation methods to reveal the dark side of SOTA LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple, low-cost, yet effective strategies for alleviation: question repeating and supervised fine-tuning. \", -1)\n                        ])),\n                        _: 1\n                      }),\n                      _createVNode(_component_el_col, { span: 10 }, {\n                        default: _withCtx(() => _cache[13] || (_cache[13] = [\n                          _createElementVNode(\"img\", {\n                            src: _imports_0,\n                            alt: \"Overview\",\n                            class: \"responsive-image abstract-image\"\n                          }, null, -1)\n                        ])),\n                        _: 1\n                      })\n                    ]),\n                    _: 1\n                  })\n                ])\n              ]),\n              _createElementVNode(\"div\", _hoisted_13, [\n                _cache[20] || (_cache[20] = _createElementVNode(\"h3\", null, [\n                  _createElementVNode(\"span\", { class: \"section-title\" }, \"Failure of Intrinsic Self-Correction\")\n                ], -1)),\n                _createElementVNode(\"div\", _hoisted_14, [\n                  _cache[15] || (_cache[15] = _createElementVNode(\"p\", null, \" Intrinsic self-correction mechanisms in state-of-the-art LLMs were expected to enhance performance by refining responses based solely on the model's inherent capabilities. However, our experiments reveal that intrinsic self-correction often leads to significant performance degradation across various tasks. \", -1)),\n                  _cache[16] || (_cache[16] = _createElementVNode(\"h4\", null, \"Experimental Results\", -1)),\n                  _cache[17] || (_cache[17] = _createElementVNode(\"p\", null, \"Below are the key experimental results demonstrating the failures of intrinsic self-correction:\", -1)),\n                  _createVNode(_component_el_table, {\n                    data: boolqData,\n                    style: {\"width\":\"100%\"},\n                    border: \"\"\n                  }, {\n                    default: _withCtx(() => [\n                      _createVNode(_component_el_table_column, {\n                        prop: \"model\",\n                        label: \"Model\",\n                        align: \"center\"\n                      }),\n                      _createVNode(_component_el_table_column, {\n                        prop: \"acc1\",\n                        label: \"ACC₁ (↓ΔACC) (%)\",\n                        align: \"center\"\n                      }),\n                      _createVNode(_component_el_table_column, {\n                        prop: \"overturned\",\n                        label: \"✓→✗ (%)\",\n                        align: \"center\"\n                      }, {\n                        default: _withCtx((scope) => [\n                          _createTextVNode(_toDisplayString(scope.row.overturned), 1)\n                        ]),\n                        _: 1\n                      })\n                    ]),\n                    _: 1\n                  }),\n                  _cache[18] || (_cache[18] = _createElementVNode(\"p\", { class: \"table-caption\" }, \"Table 1: Self-correction performance on the Yes/No question answering task.\", -1)),\n                  _cache[19] || (_cache[19] = _createElementVNode(\"div\", { class: \"observation-box\" }, [\n                    _createElementVNode(\"strong\", null, \"Observation 1:\"),\n                    _createTextVNode(\" Self-correction can fail in diverse tasks. For SOTA LLMs, self-correction failures are reduced but not solved. They are even worse in certain tasks. \")\n                  ], -1))\n                ])\n              ]),\n              _createElementVNode(\"div\", _hoisted_15, [\n                _cache[31] || (_cache[31] = _createElementVNode(\"h3\", null, [\n                  _createElementVNode(\"span\", { class: \"section-title\" }, \"Interpretation\")\n                ], -1)),\n                _createElementVNode(\"div\", _hoisted_16, [\n                  _cache[30] || (_cache[30] = _createElementVNode(\"p\", null, \" We propose three interpretation methods to understand how and why intrinsic self-correction fails in different tasks: \", -1)),\n                  _createElementVNode(\"div\", _hoisted_17, [\n                    _createVNode(_component_el_row, { gutter: 20 }, {\n                      default: _withCtx(() => [\n                        _createVNode(_component_el_col, { span: 12 }, {\n                          default: _withCtx(() => _cache[21] || (_cache[21] = [\n                            _createElementVNode(\"h4\", null, \"1. Internal Answer Wavering\", -1),\n                            _createElementVNode(\"p\", null, \" We analyze LLMs' internal token representations at each layer to track how confidence in different answers evolves. Our findings show that: \", -1),\n                            _createElementVNode(\"ul\", null, [\n                              _createElementVNode(\"li\", null, \"Self-correction increases internal answer wavering from 8.3% to 14.1%\"),\n                              _createElementVNode(\"li\", null, \"Prompting with \\\"Are you sure?\\\" produces nearly identical confidence patterns as directly stating \\\"You are wrong\\\"\")\n                            ], -1)\n                          ])),\n                          _: 1\n                        }),\n                        _createVNode(_component_el_col, { span: 12 }, {\n                          default: _withCtx(() => _cache[22] || (_cache[22] = [\n                            _createElementVNode(\"img\", {\n                              src: _imports_1,\n                              alt: \"Internal Confidence Analysis\",\n                              class: \"method-image\"\n                            }, null, -1)\n                          ])),\n                          _: 1\n                        })\n                      ]),\n                      _: 1\n                    }),\n                    _cache[23] || (_cache[23] = _createElementVNode(\"div\", { class: \"observation-box\" }, [\n                      _createElementVNode(\"strong\", null, \"Observation 2:\"),\n                      _createTextVNode(\" Self-correction causes internal answer wavering, which could further lead to wrong final answers. Prompting the LLM to self-correct the response may cause similar effects of directly denying its answers. \")\n                    ], -1))\n                  ]),\n                  _createElementVNode(\"div\", _hoisted_18, [\n                    _createVNode(_component_el_row, { gutter: 20 }, {\n                      default: _withCtx(() => [\n                        _createVNode(_component_el_col, { span: 11 }, {\n                          default: _withCtx(() => _cache[24] || (_cache[24] = [\n                            _createElementVNode(\"h4\", null, \"2. Token Attribution Analysis: Prompt Bias\", -1),\n                            _createElementVNode(\"p\", null, \" Using our PACT (Prompt Attribution and Contribution Tracking) method, we measure how different parts of the input influence the model's decisions: \", -1),\n                            _createElementVNode(\"ul\", null, [\n                              _createElementVNode(\"li\", null, \"When correct answers are overturned, models show stronger attribution to refinement prompts\"),\n                              _createElementVNode(\"li\", null, \"When correct answers are retained, models maintain focus on the original question\")\n                            ], -1)\n                          ])),\n                          _: 1\n                        }),\n                        _createVNode(_component_el_col, { span: 1 }),\n                        _createVNode(_component_el_col, { span: 11 }, {\n                          default: _withCtx(() => _cache[25] || (_cache[25] = [\n                            _createElementVNode(\"img\", {\n                              src: _imports_2,\n                              alt: \"Token Attribution Analysis\",\n                              class: \"method-image\"\n                            }, null, -1)\n                          ])),\n                          _: 1\n                        })\n                      ]),\n                      _: 1\n                    }),\n                    _cache[26] || (_cache[26] = _createElementVNode(\"div\", { class: \"observation-box\" }, [\n                      _createElementVNode(\"strong\", null, \"Observation 3:\"),\n                      _createTextVNode(\" Self-correction fails since LLMs are biased towards the refinement prompt rather than the original question. \")\n                    ], -1))\n                  ]),\n                  _createElementVNode(\"div\", _hoisted_19, [\n                    _createVNode(_component_el_row, { gutter: 20 }, {\n                      default: _withCtx(() => [\n                        _createVNode(_component_el_col, { span: 11 }, {\n                          default: _withCtx(() => _cache[27] || (_cache[27] = [\n                            _createElementVNode(\"h4\", null, \"3. Human-like Cognitive Bias Analysis\", -1),\n                            _createElementVNode(\"p\", null, \" In complex tasks, we identify three types of human-like cognitive biases that emerge during self-correction: \", -1),\n                            _createElementVNode(\"ul\", null, [\n                              _createElementVNode(\"li\", null, [\n                                _createElementVNode(\"strong\", null, \"Overthinking:\"),\n                                _createTextVNode(\" Excessive reasoning without taking correct actions (avg. 15.4 vs 5.3 \\\"think\\\" steps)\")\n                              ]),\n                              _createElementVNode(\"li\", null, [\n                                _createElementVNode(\"strong\", null, \"Cognitive Overload:\"),\n                                _createTextVNode(\" Forgetting critical information when processing long prompts\")\n                              ]),\n                              _createElementVNode(\"li\", null, [\n                                _createElementVNode(\"strong\", null, \"Perfectionism Bias:\"),\n                                _createTextVNode(\" Over-optimization leading to constraint violations\")\n                              ])\n                            ], -1)\n                          ])),\n                          _: 1\n                        }),\n                        _createVNode(_component_el_col, { span: 13 }, {\n                          default: _withCtx(() => _cache[28] || (_cache[28] = [\n                            _createElementVNode(\"img\", {\n                              src: _imports_3,\n                              alt: \"Human Cognitive Biases\",\n                              class: \"method-image\"\n                            }, null, -1)\n                          ])),\n                          _: 1\n                        })\n                      ]),\n                      _: 1\n                    }),\n                    _cache[29] || (_cache[29] = _createElementVNode(\"div\", { class: \"observation-box\" }, [\n                      _createElementVNode(\"strong\", null, \"Observation 4:\"),\n                      _createTextVNode(\" In complex tasks, LLMs exhibit human-like cognitive biases during self-correction: (1) Overthinking: LLM performs excessive \\\"think\\\" without taking correct actions; (2) Cognitive overload: LLM forgets the correct command syntax when processing long prompt; (3) Perfectionism bias: LLM wants to be more efficient, but instead violates environmental restrictions. \")\n                    ], -1))\n                  ])\n                ])\n              ]),\n              _createElementVNode(\"div\", _hoisted_20, [\n                _cache[39] || (_cache[39] = _createElementVNode(\"h3\", null, [\n                  _createElementVNode(\"span\", { class: \"section-title\" }, \"Alleviation\")\n                ], -1)),\n                _createElementVNode(\"div\", _hoisted_21, [\n                  _cache[32] || (_cache[32] = _createElementVNode(\"p\", null, \" Based on our findings that self-correction failures are mainly due to model's behavior of changing answers when meeting refinement prompts, we propose two simple yet effective strategies: \", -1)),\n                  _cache[33] || (_cache[33] = _createElementVNode(\"div\", { style: {\"text-align\":\"center\"} }, [\n                    _createElementVNode(\"img\", {\n                      src: _imports_4,\n                      alt: \"Question Repeating\",\n                      class: \"responsive-image\",\n                      style: {\"width\":\"80%\"}\n                    })\n                  ], -1)),\n                  _cache[34] || (_cache[34] = _createElementVNode(\"div\", { class: \"solution-item\" }, [\n                    _createElementVNode(\"h4\", null, \"1. Question Repeating\"),\n                    _createElementVNode(\"p\", null, \" We attach the original question to the end of the refinement prompt to reduce recency bias. For example: \\\"Are you sure? Think and answer again.\\\" → \\\"Are you sure? Think and answer again. Is human a kind of animals?\\\" \")\n                  ], -1)),\n                  _cache[35] || (_cache[35] = _createElementVNode(\"div\", { class: \"solution-item\" }, [\n                    _createElementVNode(\"h4\", null, \"2. Low-cost Supervised Fine-Tuning (SFT)\"),\n                    _createElementVNode(\"p\", null, \" We fine-tune models with extremely few samples (4 for Llama, 10 for GPT) selected from correct→wrong cases, without introducing external knowledge. The cost is only $0.004 and 3 minutes. \")\n                  ], -1)),\n                  _cache[36] || (_cache[36] = _createElementVNode(\"h4\", null, \"Key Results\", -1)),\n                  _cache[37] || (_cache[37] = _createElementVNode(\"ul\", null, [\n                    _createElementVNode(\"li\", null, \"Both strategies significantly reduce self-correction failures in Yes/No questions\"),\n                    _createElementVNode(\"li\", null, \"SFT almost eliminates all correct→wrong cases\"),\n                    _createElementVNode(\"li\", null, \"Models fine-tuned on Yes/No questions can generalize to complex tasks\")\n                  ], -1)),\n                  _createVNode(_component_el_table, {\n                    data: mitigateData,\n                    style: {\"width\":\"100%\"},\n                    border: \"\"\n                  }, {\n                    default: _withCtx(() => [\n                      _createVNode(_component_el_table_column, {\n                        prop: \"model\",\n                        label: \"Model\",\n                        align: \"center\"\n                      }),\n                      _createVNode(_component_el_table_column, {\n                        prop: \"acc1\",\n                        label: \"ACC₁ (↓ΔACC) (%)\",\n                        align: \"center\"\n                      }),\n                      _createVNode(_component_el_table_column, {\n                        prop: \"overturned\",\n                        label: \"✓→✗ (%)\",\n                        align: \"center\"\n                      }, {\n                        default: _withCtx((scope) => [\n                          _createTextVNode(_toDisplayString(scope.row.overturned), 1)\n                        ]),\n                        _: 1\n                      })\n                    ]),\n                    _: 1\n                  }),\n                  _cache[38] || (_cache[38] = _createElementVNode(\"p\", { class: \"table-caption\" }, \"Table 2: Alleviating self-correction failure on Yes/No question answering task.\", -1))\n                ])\n              ]),\n              _cache[40] || (_cache[40] = _createElementVNode(\"div\", {\n                class: \"section\",\n                id: \"resources\"\n              }, [\n                _createElementVNode(\"h3\", null, [\n                  _createElementVNode(\"span\", { class: \"section-title\" }, \"Resources\")\n                ]),\n                _createElementVNode(\"div\", { class: \"section-content\" }, [\n                  _createElementVNode(\"p\", null, \" Access our code repository through the following links: \"),\n                  _createElementVNode(\"ul\", null, [\n                    _createElementVNode(\"li\", null, [\n                      _createElementVNode(\"a\", {\n                        href: \"https://anonymous.4open.science/r/SC-15FB/\",\n                        target: \"_blank\"\n                      }, \"Project Code\")\n                    ])\n                  ])\n                ])\n              ], -1))\n            ]),\n            _: 1\n          })\n        ]),\n        _: 1\n      })\n    ])\n  ]))\n}\n}\n\n})","<template>\n    <div class=\"project-page\">\n      <!-- Navigation Bar -->\n      <el-menu\n        mode=\"horizontal\"\n        :background-color=\"'rgb(140, 21, 21)'\"\n        text-color=\"#fff\"\n        active-text-color=\"#fff\"\n      >\n        <el-menu-item index=\"/\">\n          <span style=\"font-weight: 800\">X-ISC</span>\n        </el-menu-item>\n        <el-menu-item @click=\"scrollToSection('abstract')\">Abstract</el-menu-item>\n        <el-menu-item @click=\"scrollToSection('failure')\">Failure of Intrinsic Self-Correction</el-menu-item>\n        <el-menu-item @click=\"scrollToSection('interpretation')\">Interpretation</el-menu-item>\n        <el-menu-item @click=\"scrollToSection('alleviation')\">Alleviation</el-menu-item>\n        <el-menu-item @click=\"scrollToSection('resources')\">Resources</el-menu-item>\n      </el-menu>\n  \n      <!-- Header Section -->\n      <div class=\"container header\">\n        <h2 class=\"title\">Understanding the Dark Side of LLMs' Intrinsic Self-Correction</h2>\n        <h4 class=\"subtitle\">\n          <span class=\"underline\">Ex</span>plaining \n          <span class=\"underline\">I</span>ntrinsic \n          <span class=\"underline\">S</span>elf-<span class=\"underline\">C</span>orrection \n          (X-ISC)\n        </h4>\n        \n        <div class=\"author-info\">\n          <span>Anonymous submission</span>\n        </div>\n  \n        <!-- 添加 GitHub 链接按钮 -->\n        <div class=\"github-link-container\">\n          <a href=\"https://anonymous.4open.science/r/SC-15FB/\" \n             class=\"github-button\"\n             target=\"_blank\">\n            <i class=\"github-icon\">\n              <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"currentColor\" viewBox=\"0 0 16 16\">\n                <path d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z\"/>\n              </svg>\n            </i>\n            <span>Project Code</span>\n          </a>\n        </div>\n      </div>\n  \n      <!-- After github-link-container div and before main-content div -->\n      <div class=\"video-section\">\n        <el-row :gutter=\"10\">\n          <el-col :span=\"7\">\n            <div class=\"video-options\">\n              <div class=\"video-selector-list\">\n                <div class=\"video-option-header\">\n                  Select Model & Question:\n                </div>\n                <div \n                  v-for=\"option in videoOptions\" \n                  :key=\"option.value\"\n                  class=\"video-option-item\"\n                  :class=\"{ active: selectedVideo === option.value }\"\n                  @click=\"selectedVideo = option.value\"\n                >\n                  <div class=\"model-name\">{{ option.model }}</div>\n                  <div class=\"question\">{{ option.question }}</div>\n                </div>\n              </div>\n            </div>\n          </el-col>\n          <el-col :span=\"15\">\n            <div class=\"video-player\">\n              <video \n                :src=\"currentVideoSrc\" \n                controls \n                class=\"demo-video\"\n                v-if=\"currentVideoSrc\"\n              >\n                Your browser does not support the video tag.\n              </video>\n            </div>\n          </el-col>\n        </el-row>\n      </div>\n  \n      <!-- Main Content -->\n      <div class=\"container main-content\">\n        <el-row justify=\"center\">\n          <el-col :span=\"20\">\n            <!-- Abstract Section -->\n            <div class=\"section\" id=\"abstract\">\n              <h3>\n                <span class=\"section-title\">Abstract</span>\n              </h3>\n              <div class=\"section-content\">\n                <el-row :gutter=\"20\">\n                    <el-col :span=\"1\"></el-col>\n                  <el-col :span=\"11\">\n                    <p>\n                      Intrinsic self-correction was proposed to improve LLMs' responses via feedback solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases? By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design interpretation methods to reveal the dark side of SOTA LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple, low-cost, yet effective strategies for alleviation: question repeating and supervised fine-tuning.\n                    </p>\n                  </el-col>\n                <el-col :span=\"10\">\n                    <img \n                        src=\"@/figures/overviewf3.png\" \n                        alt=\"Overview\" \n                        class=\"responsive-image abstract-image\"\n                    />\n                    </el-col>\n                </el-row>\n              </div>\n            </div>\n  \n            <!-- Failure of Intrinsic Self-Correction Section -->\n            <div class=\"section\" id=\"failure\">\n              <h3>\n                <span class=\"section-title\">Failure of Intrinsic Self-Correction</span>\n              </h3>\n              <div class=\"section-content\">\n                <p>\n                  Intrinsic self-correction mechanisms in state-of-the-art LLMs were expected to enhance performance by refining responses based solely on the model's inherent capabilities. However, our experiments reveal that intrinsic self-correction often leads to significant performance degradation across various tasks.\n                </p>\n  \n                <!-- Experimental Tables Placeholder -->\n                <h4>Experimental Results</h4>\n                <p>Below are the key experimental results demonstrating the failures of intrinsic self-correction:</p>\n                \n                <!-- Example Table for Yes/No Question Answering Task -->\n                <el-table :data=\"boolqData\" style=\"width: 100%\" border>\n                  <el-table-column prop=\"model\" label=\"Model\" align=\"center\">\n                  </el-table-column>\n                  <el-table-column prop=\"acc1\" label=\"ACC₁ (↓ΔACC) (%)\" align=\"center\">\n                  </el-table-column>\n                  <el-table-column prop=\"overturned\" label=\"✓→✗ (%)\" align=\"center\">\n                    <template #default=\"scope\">\n                      {{ scope.row.overturned }}\n                    </template>\n                  </el-table-column>\n                </el-table>\n                \n                <p class=\"table-caption\">Table 1: Self-correction performance on the Yes/No question answering task.</p>\n  \n                <!-- Additional Tables for Complex Tasks -->\n                <!-- Repeat similar <el-table> components for other tasks as needed -->\n                \n                <!-- Observation 1 -->\n                <div class=\"observation-box\">\n                  <strong>Observation 1:</strong> Self-correction can fail in diverse tasks. For SOTA LLMs, self-correction failures are reduced but not solved. They are even worse in certain tasks.\n                </div>\n              </div>\n            </div>\n  \n            <!-- Interpretation Section -->\n            <div class=\"section\" id=\"interpretation\">\n              <h3>\n                <span class=\"section-title\">Interpretation</span>\n              </h3>\n              <div class=\"section-content\">\n                <p>\n                  We propose three interpretation methods to understand how and why intrinsic self-correction fails in different tasks:\n                </p>\n  \n                <!-- Method 1: Mechanistic Interpretability -->\n                <div class=\"method-box\">\n                  <el-row :gutter=\"20\">\n                    <el-col :span=\"12\">\n                      <h4>1. Internal Answer Wavering</h4>\n                      <p>\n                        We analyze LLMs' internal token representations at each layer to track how confidence in different answers evolves. Our findings show that:\n                      </p>\n                      <ul>\n                        <li>Self-correction increases internal answer wavering from 8.3% to 14.1%</li>\n                        <li>Prompting with \"Are you sure?\" produces nearly identical confidence patterns as directly stating \"You are wrong\"</li>\n                      </ul>\n                    </el-col>\n                    <el-col :span=\"12\">\n                      <img \n                        src=\"@/figures/internal_confidence.png\" \n                        alt=\"Internal Confidence Analysis\" \n                        class=\"method-image\"\n                      />\n                    </el-col>\n                  </el-row>\n                  <div class=\"observation-box\">\n                    <strong>Observation 2:</strong> Self-correction causes internal answer wavering, which could further lead to wrong final answers. Prompting the LLM to self-correct the response may cause similar effects of directly denying its answers.\n                  </div>\n                </div>\n  \n                <!-- Method 2: Token Attribution -->\n                <div class=\"method-box\">\n                  <el-row :gutter=\"20\">\n                    <el-col :span=\"11\">\n                      <h4>2. Token Attribution Analysis: Prompt Bias</h4>\n                      <p>\n                        Using our PACT (Prompt Attribution and Contribution Tracking) method, we measure how different parts of the input influence the model's decisions:\n                      </p>\n                      <ul>\n                        <li>When correct answers are overturned, models show stronger attribution to refinement prompts</li>\n                        <li>When correct answers are retained, models maintain focus on the original question</li>\n                      </ul>\n                    </el-col>\n                    <el-col :span=\"1\"></el-col>\n                    <el-col :span=\"11\">\n                      <img \n                        src=\"@/figures/case.png\" \n                        alt=\"Token Attribution Analysis\" \n                        class=\"method-image\"\n                      />\n                    </el-col>\n                  </el-row>\n                  <div class=\"observation-box\">\n                    <strong>Observation 3:</strong> Self-correction fails since LLMs are biased towards the refinement prompt rather than the original question. \n                  </div>\n                </div>\n  \n                <!-- Method 3: Human-like Cognitive Bias -->\n                <div class=\"method-box\">\n                  <el-row :gutter=\"20\">\n                    <el-col :span=\"11\">\n                      <h4>3. Human-like Cognitive Bias Analysis</h4>\n                      <p>\n                        In complex tasks, we identify three types of human-like cognitive biases that emerge during self-correction:\n                      </p>\n                      <ul>\n                        <li><strong>Overthinking:</strong> Excessive reasoning without taking correct actions (avg. 15.4 vs 5.3 \"think\" steps)</li>\n                        <li><strong>Cognitive Overload:</strong> Forgetting critical information when processing long prompts</li>\n                        <li><strong>Perfectionism Bias:</strong> Over-optimization leading to constraint violations</li>\n                      </ul>\n                    </el-col>\n                    <el-col :span=\"13\">\n                      <img \n                        src=\"@/figures/humanCognitiveBiasf.png\" \n                        alt=\"Human Cognitive Biases\" \n                        class=\"method-image\"\n                      />\n                    </el-col>\n                  </el-row>\n                  <div class=\"observation-box\">\n                    <strong>Observation 4:</strong> In complex tasks, LLMs exhibit human-like cognitive biases during self-correction: (1) Overthinking: LLM performs excessive \"think\" without taking correct actions; (2) Cognitive overload: LLM forgets the correct command syntax when processing long prompt; (3) Perfectionism bias: LLM wants to be more efficient, but instead violates environmental restrictions.\n                  </div>\n                </div>\n              </div>\n            </div>\n  \n            <!-- Solutions Section -->\n            <div class=\"section\" id=\"alleviation\">\n              <h3>\n                <span class=\"section-title\">Alleviation</span>\n              </h3>\n              <div class=\"section-content\">\n                <p>\n                  Based on our findings that self-correction failures are mainly due to model's behavior of changing answers when meeting refinement prompts, we propose two simple yet effective strategies:\n                </p>\n                <div style=\"text-align: center;\">\n                  <img src=\"@/figures/mergedMitigation2.png\" alt=\"Question Repeating\" class=\"responsive-image\" style=\"width: 80%;\" />\n                </div>\n  \n                <!-- Mitigation Strategy 1: Question Repeating -->\n                <div class=\"solution-item\">\n                  <h4>1. Question Repeating</h4>\n                  <p>\n                    We attach the original question to the end of the refinement prompt to reduce recency bias. For example:\n                    \"Are you sure? Think and answer again.\" → \"Are you sure? Think and answer again. Is human a kind of animals?\"\n                  </p>\n                </div>\n  \n                <!-- Mitigation Strategy 2: Supervised Fine-Tuning -->\n                <div class=\"solution-item\">\n                  <h4>2. Low-cost Supervised Fine-Tuning (SFT)</h4>\n                  <p>\n                    We fine-tune models with extremely few samples (4 for Llama, 10 for GPT) selected from correct→wrong cases, without introducing external knowledge. The cost is only $0.004 and 3 minutes.\n                  </p>\n                </div>\n  \n                <!-- Results -->\n                <h4>Key Results</h4>\n                <ul>\n                  <li>Both strategies significantly reduce self-correction failures in Yes/No questions</li>\n                  <li>SFT almost eliminates all correct→wrong cases</li>\n                  <li>Models fine-tuned on Yes/No questions can generalize to complex tasks</li>\n                </ul>\n  \n                <!-- Results Tables -->\n                <el-table :data=\"mitigateData\" style=\"width: 100%\" border>\n                  <el-table-column prop=\"model\" label=\"Model\" align=\"center\">\n                  </el-table-column>\n                  <el-table-column prop=\"acc1\" label=\"ACC₁ (↓ΔACC) (%)\" align=\"center\">\n                  </el-table-column>\n                  <el-table-column prop=\"overturned\" label=\"✓→✗ (%)\" align=\"center\">\n                    <template #default=\"scope\">\n                      {{ scope.row.overturned }}\n                    </template>\n                  </el-table-column>\n                </el-table>\n                <p class=\"table-caption\">Table 2: Alleviating self-correction failure on Yes/No question answering task.</p>\n              </div>\n            </div>\n  \n            <!-- Resources Section -->\n            <div class=\"section\" id=\"resources\">\n              <h3>\n                <span class=\"section-title\">Resources</span>\n              </h3>\n              <div class=\"section-content\">\n                <p>\n                  Access our code repository through the following links:\n                </p>\n                <ul>\n                  <li><a href=\"https://anonymous.4open.science/r/SC-15FB/\" target=\"_blank\">Project Code</a></li>\n                </ul>\n              </div>\n            </div>\n          </el-col>\n        </el-row>\n      </div>\n    </div>\n  </template>\n  \n  <script setup lang=\"ts\">\n  // No additional scripts needed for static content\n  \n  // Sample Data for Tables\n  const boolqData = [\n    { \n      model: 'ChatGPT o1-preview', \n      acc1: '78.7 (↓4.9)', \n      overturned: '13.2' \n    },\n    { \n      model: 'ChatGPT o1-mini', \n      acc1: '74.1 (↓4.2)', \n      overturned: '15.6' \n    },\n    { \n      model: 'ChatGPT 4o', \n      acc1: '79.2 (↓4.9)', \n      overturned: '11.3' \n    },\n    { \n      model: 'ChatGPT 3.5-turbo', \n      acc1: '62.5 (↓12.1)', \n      overturned: '34.0' \n    },\n    { \n      model: 'Llama-3.1-8B', \n      acc1: '49.2 (↓20.4)', \n      overturned: '58.8' \n    },\n    { \n      model: 'Llama-3-8B', \n      acc1: '50.1 (↓20.3)', \n      overturned: '58.2' \n    },\n    { \n      model: 'Llama-2-7B', \n      acc1: '52.8 (↓8.7)', \n      overturned: '26.5' \n    }\n  ];\n  \n  const mitigateData = [\n    { model: 'GPT-4o', acc1: '79.2 (↓4.9)', overturned: '11.3' },\n    { model: 'GPT-4o + Question repeating', acc1: '83.6 (↓0.5)', overturned: '6.0' },\n    { model: 'GPT-4o + SFT', acc1: '87.7 (↑4.1)', overturned: '0' },\n    { model: 'GPT-3.5-turbo', acc1: '62.5 (↓12.1)', overturned: '34.0' },\n    { model: 'GPT-3.5-turbo + Question repeating', acc1: '67.4 (↓7.2)', overturned: '23.1' },\n    { model: 'GPT-3.5-turbo + SFT', acc1: '76.2 (↑1.6)', overturned: '0' },\n    { model: 'Llama-3.1-8B', acc1: '49.2 (↓20.4)', overturned: '58.8' },\n    { model: 'Llama-3.1-8B + Question repeating', acc1: '52.4 (↓17.2)', overturned: '52.8' },\n    { model: 'Llama-3.1-8B + SFT', acc1: '70.3 (↑0.7)', overturned: '0' }\n  ];\n  \n  const scrollToSection = (sectionId: string) => {\n    const element = document.getElementById(sectionId);\n    if (element) {\n      element.scrollIntoView({ behavior: 'smooth' });\n    }\n  };\n  \n  import { ref, computed } from 'vue';\n  const video4o = new URL('@/figures/4o.mov', import.meta.url).href\n  const video4oMini = new URL('@/figures/4o-mini.mov', import.meta.url).href\n  const videoO1 = new URL('@/figures/o1.mov', import.meta.url).href\n  const videoO1Mini = new URL('@/figures/o1-mini.mov', import.meta.url).href\n  \n  const videoOptions = [\n    {\n      value: '4o',\n      label: 'GPT-4o - Population Question',\n      model: 'ChatGPT 4o (2024.12.17)',\n      question: 'Does China has more population than India?',\n      src: video4o\n    },\n    {\n      value: '4o-mini',\n      label: 'GPT-4o - Moon Jump Question',\n      model: 'ChatGPT 4o mini (2024.12.17)',\n      question: 'Can I jump from Earth to Moon?',\n      src: video4oMini\n    },\n    {\n      value: 'o1',\n      label: 'GPT-o1 - Population Question',\n      model: 'ChatGPT o1 (2024.12.17)',\n      question: 'Does China has more population than India?',\n      src: videoO1\n    },\n    {\n      value: 'o1-mini',\n      label: 'GPT-o1 - Arms Question',\n      model: 'ChatGPT o1-mini (2024.12.17)',\n      question: 'Does human have three arms?',\n      src: videoO1Mini\n    }\n  ];\n  \n  const selectedVideo = ref('4o');\n  \n  const currentVideoSrc = computed(() => {\n    const option = videoOptions.find(opt => opt.value === selectedVideo.value);\n    return option ? option.src : '';\n  });\n  </script>\n  \n  <style scoped>\n  .project-page {\n    min-height: 100vh;\n    background-color: #fff;\n  }\n  \n  .container {\n    padding: 0 20px;\n    max-width: 1200px;\n    margin: 0 auto;\n  }\n  \n  .header {\n    text-align: center;\n    padding: 20px 0 0 0;\n  }\n  \n  .title {\n    margin-bottom: 0;\n    font-size: 2em;\n    font-weight: normal;\n  }\n  \n  .subtitle {\n    color: rgb(140, 21, 21);\n    margin-top: 5px;\n    margin-bottom: 5px;\n    font-weight: normal;\n    font-size: 1.5em;\n  }\n  \n  .section {\n    margin: 15px 0;\n    padding: 0px;\n    background-color: transparent;\n    border-radius: 0;\n    box-shadow: none;\n  }\n  \n  .section-title {\n    color: rgb(140, 21, 21);\n    font-size: 22px;\n    display: block;\n    margin-bottom: 10px;\n    border-bottom: 2px solid rgb(140, 21, 21);\n    padding-bottom: 6px;\n  }\n  \n  .section-content {\n    margin-top: 15px;\n  }\n  \n  .observation-box {\n    margin: 15px 0;\n    padding: 12px;\n    background-color: #f0f8ff;\n    border-left: 4px solid rgb(140, 21, 21);\n    border-radius: 4px;\n  }\n  \n  .table-caption {\n    text-align: center;\n    font-size: 0.85em;\n    color: #555;\n    margin: 5px auto 15px;\n    width: 80%;\n  }\n  \n  .solution-item {\n    margin-bottom: 15px;\n    padding: 15px;\n    background-color: #f8f8f8;\n    border-radius: 8px;\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n  }\n  \n  .solution-item h4 {\n    color: rgb(140, 21, 21);\n    margin-top: 0;\n    margin-bottom: 10px;\n  }\n  \n  .responsive-image {\n    width: 100%;\n    height: auto;\n    margin-top: 15px;\n    border-radius: 8px;\n  }\n  \n  /* 移除之前的 overview 图片特殊样式 */\n  #abstract .responsive-image {\n    max-width: 100%;\n    margin: 0;\n  }\n  \n  /* 添加新的 abstract 图片样式 */\n  .abstract-image {\n    width: 100%;\n    height: auto;\n    display: block;\n    margin: 15px 0;\n  }\n  \n  /* 为双栏布局加响应式设计 */\n  @media (max-width: 768px) {\n    .finding-item, .method-item, .result-item, .solution-item {\n      padding: 15px;\n    }\n  \n    .section-title {\n      font-size: 22px;\n    }\n  \n    .title {\n      font-size: 1.8em;\n    }\n  \n    .subtitle {\n      font-size: 1.3em;\n    }\n  \n    #abstract .el-row {\n      display: flex;\n      flex-direction: column;\n    }\n  \n    #abstract .el-col {\n      width: 100%;\n      margin-bottom: 20px;\n    }\n  }\n  \n  .github-link-container {\n    text-align: center;\n    margin: 10px 0 0 0;\n  }\n  \n  .github-button {\n    display: inline-flex;\n    align-items: center;\n    gap: 8px;\n    background-color: rgb(140, 21, 21);\n    color: white;\n    padding: 8px 15px;\n    border-radius: 20px;\n    text-decoration: none;\n    font-size: 14px;\n    transition: background-color 0.3s;\n  }\n  \n  .github-button:hover {\n    background-color: rgb(120, 18, 18);\n    color: white;\n    text-decoration: none;\n  }\n  \n  .github-icon {\n    display: flex;\n    align-items: center;\n  }\n  \n  :deep(.el-menu-item) {\n    font-size: 18px;  /* 增加字体大小 */\n    font-weight: 400; /* 稍微加粗一点 */\n  }\n  \n  :deep(.el-menu-item:first-child) {\n    font-size: 20px;\n    font-weight: 800;\n  }\n  \n  /* 修改表格样式 */\n  :deep(.el-table) {\n    margin: 20px auto;\n    width: 80% !important;\n    font-size: 14px;\n  }\n  \n  :deep(.el-table__header) {\n    font-weight: bold;\n    font-size: 14px;\n  }\n  \n  :deep(.el-table__cell) {\n    text-align: center !important;\n    padding: 6px 0;\n  }\n  \n  :deep(.el-table .cell) {\n    padding: 4px;\n    white-space: nowrap;\n  }\n  \n  /* 调整标题和段落的间距 */\n  h4 {\n    margin-top: 15px;\n    margin-bottom: 10px;\n    font-size: 18px; /* 减小二级标题大小 */\n  }\n  \n  p {\n    margin: 8px 0; /* 减小段落间距 */\n    line-height: 1.5; /* 减小行 */\n  }\n  \n  /* 添加新的样式类用于中等大小的图片 */\n  .medium-image {\n    width: 70%;  /* 设置为容器宽度的70% */\n    display: block;\n    margin: 15px auto;  /* 上下间距15px，左右自动居中 */\n  }\n  \n  .method-box {\n    margin: 25px 0;\n    padding: 20px;\n    background-color: #f8f8f8;\n    border-radius: 8px;\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n  }\n  \n  .method-box h4 {\n    color: rgb(140, 21, 21);\n    margin-top: 0;\n    margin-bottom: 15px;\n  }\n  \n  .method-image {\n    width: 100%;\n    height: auto;\n    border-radius: 8px;\n    margin-top: 10px;\n  }\n  \n  /* 添加以下样式来调整列表项的左边距 */\n  ul {\n    padding-left: 20px; /* 减默认的左边距 */\n    margin: 8px 0;\n  }\n  \n  li {\n    margin: 4px 0; /* 调整列表项之间的垂直间距 */\n  }\n  \n  /* 添加新的样式规则 */\n  #abstract {\n    margin-top: 0;\n    padding-top: 0;\n  }\n  \n  .underline {\n    text-decoration: underline;\n  }\n  \n  .gif-container {\n    margin: 15px auto 30px;\n    text-align: center;\n    display: flex;\n    justify-content: center;\n    gap: 30px;\n  }\n  \n  .gif-column {\n    display: flex;\n    justify-content: center;\n  }\n  \n  .gif-image {\n    width: 100%;\n    max-width: 450px;\n    height: auto;\n    border-radius: 8px;\n    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);\n  }\n  \n  .video-section {\n    margin: 20px auto;\n    width: 90%;\n    max-width: 950px;\n    display: flex;\n    justify-content: center;\n  }\n  \n  .video-section .el-row {\n    width: 100%;\n  }\n  .video-options, .video-player {\n    margin: 0;\n    padding: 0;\n  }\n  \n  .video-options {\n    height: 100%;\n  }\n  \n  .video-selector-list {\n    border: 1px solid #ddd;\n    border-radius: 4px;\n    height: 700px;\n    display: flex;\n    flex-direction: column;\n  }\n  \n  .video-option-header {\n    padding: 15px;\n    font-weight: bold;\n    background-color: #f8f8f8;\n    border-bottom: 1px solid #ddd;\n    color: rgb(140, 21, 21);\n  }\n  \n  .video-option-item {\n    padding: 15px;\n    cursor: pointer;\n    border-bottom: 1px solid #ddd;\n    transition: background-color 0.2s;\n    flex: 1;\n    display: flex;\n    flex-direction: column;\n    justify-content: center;\n  }\n  \n  .video-option-item:last-child {\n    border-bottom: none;\n  }\n  \n  .video-option-item:hover {\n    background-color: #f5f5f5;\n  }\n  \n  .video-option-item.active {\n    background-color: #f0f0f0;\n    border-left: 3px solid rgb(140, 21, 21);\n  }\n  \n  .model-name {\n    font-weight: bold;\n    color: #333;\n    margin-bottom: 6px;\n    font-size: 0.95em;\n  }\n  \n  .question {\n    font-size: 0.9em;\n    color: #666;\n  }\n  \n  .video-player {\n    display: flex;\n    justify-content: flex-start;\n    align-items: flex-start;\n    height: 700px;\n    border-radius: 4px;\n  }\n  \n  .demo-video {\n    width: 100%;\n    height: 100%;\n    border-radius: 4px;\n    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);\n    object-fit: contain;\n  }\n  \n  .video-player:empty::before {\n    content: \"Loading video...\";\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    width: 100%;\n    height: 100%;\n    color: #666;\n  }\n  </style>\n  ","import script from \"./ProjectView.vue?vue&type=script&setup=true&lang=ts\"\nexport * from \"./ProjectView.vue?vue&type=script&setup=true&lang=ts\"\n\nimport \"./ProjectView.vue?vue&type=style&index=0&id=3a126768&scoped=true&lang=css\"\n\nimport exportComponent from \"../../node_modules/vue-loader/dist/exportHelper.js\"\nconst __exports__ = /*#__PURE__*/exportComponent(script, [['__scopeId',\"data-v-3a126768\"]])\n\nexport default __exports__"],"names":["_hoisted_1","class","_hoisted_2","_hoisted_3","_hoisted_4","_hoisted_5","_hoisted_6","_hoisted_7","_hoisted_8","_hoisted_9","_hoisted_10","_hoisted_11","id","_hoisted_12","_hoisted_13","_hoisted_14","_hoisted_15","_hoisted_16","_hoisted_17","_hoisted_18","_hoisted_19","_hoisted_20","_hoisted_21","_defineComponent","__name","setup","__props","boolqData","model","acc1","overturned","mitigateData","scrollToSection","sectionId","element","document","getElementById","scrollIntoView","behavior","video4o","URL","href","video4oMini","videoO1","videoO1Mini","videoOptions","value","label","question","src","selectedVideo","ref","currentVideoSrc","computed","option","find","opt","_ctx","_cache","_component_el_menu_item","_resolveComponent","_component_el_menu","_component_el_col","_component_el_row","_component_el_table_column","_component_el_table","_openBlock","_createElementBlock","_createVNode","mode","default","_withCtx","index","_createElementVNode","style","_","onClick","$event","_createTextVNode","_createStaticVNode","gutter","span","_Fragment","_renderList","key","_normalizeClass","active","_toDisplayString","controls","_createCommentVNode","justify","_imports_0","alt","data","border","prop","align","scope","row","_imports_1","_imports_2","_imports_3","_imports_4","target","__exports__"],"sourceRoot":""}