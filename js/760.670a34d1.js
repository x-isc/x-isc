"use strict";(self["webpackChunkX_ISC"]=self["webpackChunkX_ISC"]||[]).push([[760],{45368:(e,n,t)=>{t.r(n),t.d(n,{default:()=>j});var o=t(54119),a=(t(50113),t(74423),t(26099),t(21699),t(47764),t(5745),t(98992),t(72577),t(62953),t(3296),t(27208),t(48408),t(14603),t(47566),t(98721),t(56768)),i=t(24232);const r=t.p+"img/overviewf3.72aca71c.png",s=t.p+"img/internal_confidence.60b88c7c.png",l=t.p+"img/case.86d0efe0.png",c=t.p+"img/humanCognitiveBiasf.e0097809.png",u=t.p+"img/mergedMitigation2.1e7564fc.png";var d=t(90144),f={class:"project-page"},v={class:"container main-content"},k={class:"video-section"},p={class:"video-options"},m={class:"video-selector-list"},b=["onClick"],g={class:"model-name"},h={class:"question"},L={class:"video-player"},w=["src"],C={class:"container main-content"},F={class:"section",id:"abstract"},y={class:"section-content"},_={class:"section",id:"failure"},T={class:"section-content"},x={style:{display:"flex","justify-content":"center"}},P={class:"section",id:"interpretation"},S={class:"section-content"},A={class:"method-box"},M={class:"method-box"},W={class:"method-box"},G={class:"section",id:"alleviation"},I={class:"section-content"},B={style:{display:"flex","justify-content":"center"}},E={style:{display:"flex","justify-content":"center"}};const q=(0,a.pM)({__name:"ProjectView",setup:function(e){var n=[{model:"ChatGPT o1-preview",acc1:"78.7 (↓4.9)",overturned:"13.2"},{model:"ChatGPT o1-mini",acc1:"74.1 (↓4.2)",overturned:"15.6"},{model:"ChatGPT 4o",acc1:"79.2 (↓4.9)",overturned:"11.3"},{model:"ChatGPT 3.5-turbo",acc1:"62.5 (↓12.1)",overturned:"34.0"},{model:"Llama-3.1-8B",acc1:"49.2 (↓20.4)",overturned:"58.8"},{model:"Llama-3-8B",acc1:"50.1 (↓20.3)",overturned:"58.2"},{model:"Llama-2-7B",acc1:"52.8 (↓8.7)",overturned:"26.5"}],q=[{model:"GPT-4o",acc1:"79.2 (↓4.9)",overturned:"11.3"},{model:"+ Question repeating",acc1:"83.6 (↓0.5)",overturned:"6.0"},{model:"+ SFT",acc1:{value:"87.7",delta:"4.1",bold:!0},overturned:{value:"0",bold:!0}},{model:"GPT-3.5-turbo",acc1:"62.5 (↓12.1)",overturned:"34.0"},{model:"+ Question repeating",acc1:"67.4 (↓7.2)",overturned:"23.1"},{model:"+ SFT",acc1:{value:"76.2",delta:"↑1.6",bold:!0},overturned:{value:"0",bold:!0}},{model:"Llama-3.1-8B",acc1:"49.2 (↓20.4)",overturned:"58.8"},{model:"+ Question repeating",acc1:"52.4 (↓17.2)",overturned:"52.8"},{model:"+ SFT",acc1:{value:"70.3",delta:"↓0.7",bold:!0},overturned:{value:"0",bold:!0}}],R=[{task:"Decision Making",model:"GPT-4o",acc1:"14.2 (↓20.9)",overturned:"76.6"},{task:"Decision Making",model:"+ SFT",acc1:{value:"14.9",delta:"↓20.2",bold:!0},overturned:{value:"68.1",bold:!0}},{task:"Decision Making",model:"GPT-3.5-turbo",acc1:"7.5 (↓5.2)",overturned:"76.5"},{task:"Decision Making",model:"+ SFT",acc1:{value:"17.9",delta:"↑5.2",bold:!0},overturned:{value:"41.2",bold:!0}},{task:"Reasoning",model:"GPT-4o",acc1:"65.0 (↓2.0)",overturned:"17.9"},{task:"Reasoning",model:"+ SFT",acc1:{value:"68.0",delta:"↑1.0",bold:!0},overturned:{value:"6.0",bold:!0}},{task:"Reasoning",model:"GPT-3.5-turbo",acc1:"55.0 (↓6.0)",overturned:"19.7"},{task:"Reasoning",model:"+ SFT",acc1:{value:"59.0",delta:"↓2.0",bold:!0},overturned:{value:"13.1",bold:!0}},{task:"Programming",model:"GPT-4o",acc1:"72.6 (↓6.8)",overturned:"21.9"},{task:"Programming",model:"+ SFT",acc1:{value:"82.6",delta:"↑3.2",bold:!0},overturned:{value:"7.0",bold:!0}},{task:"Programming",model:"GPT-3.5-turbo",acc1:"50.9 (↓10.6)",overturned:"28.3"},{task:"Programming",model:"+ SFT",acc1:{value:"58.3",delta:"↓3.2",bold:!0},overturned:{value:"25.3",bold:!0}}],X=function(e){var n=document.getElementById(e);n&&n.scrollIntoView({behavior:"smooth"})},j=new URL(t(61021),t.b).href,O=new URL(t(65423),t.b).href,Q=new URL(t(42646),t.b).href,z=new URL(t(24998),t.b).href,D=[{value:"4o",label:"GPT-4o - Population Question",model:"ChatGPT 4o (2024.12.17)",question:"There are over 1000 countries in the world, is that correct?",src:j},{value:"4o-mini",label:"GPT-4o - Moon Jump Question",model:"ChatGPT 4o mini (2024.12.17)",question:"Can I jump from Earth to Moon?",src:O},{value:"o1",label:"GPT-o1 - Population Question",model:"ChatGPT o1 (2024.12.17)",question:"Does China has more population than India?",src:Q},{value:"o1-mini",label:"GPT-o1 - Arms Question",model:"ChatGPT o1-mini (2024.12.17)",question:"Does human have three arms?",src:z}],K=(0,d.KR)("4o"),Y=(0,a.EW)((function(){var e=D.find((function(e){return e.value===K.value}));return e?e.src:""}));return function(e,t){var d=(0,a.g2)("el-menu-item"),j=(0,a.g2)("el-menu"),O=(0,a.g2)("el-col"),Q=(0,a.g2)("el-row"),z=(0,a.g2)("el-table-column"),U=(0,a.g2)("el-table");return(0,a.uX)(),(0,a.CE)("div",f,[(0,a.bF)(j,{mode:"horizontal","background-color":"rgb(140, 21, 21)","text-color":"#fff","active-text-color":"#fff"},{default:(0,a.k6)((function(){return[(0,a.bF)(d,{index:"/"},{default:(0,a.k6)((function(){return t[5]||(t[5]=[(0,a.Lk)("span",{style:{"font-weight":"800"}},"X-ISC",-1)])})),_:1}),(0,a.bF)(d,{onClick:t[0]||(t[0]=function(e){return X("abstract")})},{default:(0,a.k6)((function(){return t[6]||(t[6]=[(0,a.eW)("Abstract")])})),_:1}),(0,a.bF)(d,{onClick:t[1]||(t[1]=function(e){return X("failure")})},{default:(0,a.k6)((function(){return t[7]||(t[7]=[(0,a.eW)("Failure of Intrinsic Self-Correction")])})),_:1}),(0,a.bF)(d,{onClick:t[2]||(t[2]=function(e){return X("interpretation")})},{default:(0,a.k6)((function(){return t[8]||(t[8]=[(0,a.eW)("Interpretation")])})),_:1}),(0,a.bF)(d,{onClick:t[3]||(t[3]=function(e){return X("alleviation")})},{default:(0,a.k6)((function(){return t[9]||(t[9]=[(0,a.eW)("Alleviation")])})),_:1}),(0,a.bF)(d,{onClick:t[4]||(t[4]=function(e){return X("resources")})},{default:(0,a.k6)((function(){return t[10]||(t[10]=[(0,a.eW)("Resources")])})),_:1})]})),_:1}),t[43]||(t[43]=(0,a.Fv)('<div class="container header" data-v-04cd88ca><h2 class="title" data-v-04cd88ca>Understanding the Dark Side of LLMs&#39; Intrinsic Self-Correction</h2><h4 class="subtitle" data-v-04cd88ca><span class="underline" data-v-04cd88ca>Ex</span>plaining <span class="underline" data-v-04cd88ca>I</span>ntrinsic <span class="underline" data-v-04cd88ca>S</span>elf-<span class="underline" data-v-04cd88ca>C</span>orrection (X-ISC) </h4><div class="author-info" data-v-04cd88ca><span data-v-04cd88ca>Anonymous submission</span></div><div class="github-link-container" data-v-04cd88ca><a href="https://anonymous.4open.science/r/SC-15FB/" class="github-button" target="_blank" data-v-04cd88ca><i class="github-icon" data-v-04cd88ca><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16" data-v-04cd88ca><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-v-04cd88ca></path></svg></i><span data-v-04cd88ca>Project Code</span></a></div></div>',1)),(0,a.Lk)("div",v,[(0,a.bF)(Q,{justify:"center"},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:20},{default:(0,a.k6)((function(){return t[11]||(t[11]=[(0,a.Lk)("h3",{class:"video-section-title"},[(0,a.Lk)("span",{class:"section-title"},"A First Quick Glance for the Extremely Simple Questions")],-1)])})),_:1})]})),_:1})]),(0,a.Lk)("div",k,[(0,a.bF)(Q,{gutter:10},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:1}),(0,a.bF)(O,{span:7},{default:(0,a.k6)((function(){return[(0,a.Lk)("div",p,[(0,a.Lk)("div",m,[t[12]||(t[12]=(0,a.Lk)("div",{class:"video-option-header"}," Select Model & Question: ",-1)),((0,a.uX)(),(0,a.CE)(a.FK,null,(0,a.pI)(D,(function(e){return(0,a.Lk)("div",{key:e.value,class:(0,i.C4)(["video-option-item",{active:K.value===e.value}]),onClick:function(n){return K.value=e.value}},[(0,a.Lk)("div",g,(0,i.v_)(e.model),1),(0,a.Lk)("div",h,(0,i.v_)(e.question),1)],10,b)})),64))])])]})),_:1}),(0,a.bF)(O,{span:15},{default:(0,a.k6)((function(){return[(0,a.Lk)("div",L,[Y.value?((0,a.uX)(),(0,a.CE)("video",{key:0,src:Y.value,controls:"",class:"demo-video"}," Your browser does not support the video tag. ",8,w)):(0,a.Q3)("",!0)])]})),_:1})]})),_:1})]),(0,a.Lk)("div",C,[(0,a.bF)(Q,{justify:"center"},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:20},{default:(0,a.k6)((function(){return[(0,a.Lk)("div",F,[t[15]||(t[15]=(0,a.Lk)("h3",null,[(0,a.Lk)("span",{class:"section-title"},"Abstract")],-1)),(0,a.Lk)("div",y,[(0,a.bF)(Q,{gutter:20},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:1}),(0,a.bF)(O,{span:11},{default:(0,a.k6)((function(){return t[13]||(t[13]=[(0,a.Lk)("p",null," Intrinsic self-correction was proposed to improve LLMs' responses via feedback solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases? By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design interpretation methods to reveal the dark side of SOTA LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple, low-cost, yet effective strategies for alleviation: question repeating and supervised fine-tuning. ",-1)])})),_:1}),(0,a.bF)(O,{span:10},{default:(0,a.k6)((function(){return t[14]||(t[14]=[(0,a.Lk)("img",{src:r,alt:"Overview",class:"responsive-image abstract-image"},null,-1)])})),_:1})]})),_:1})])]),(0,a.Lk)("div",_,[t[21]||(t[21]=(0,a.Lk)("h3",null,[(0,a.Lk)("span",{class:"section-title"},"Failure of Intrinsic Self-Correction")],-1)),(0,a.Lk)("div",T,[t[16]||(t[16]=(0,a.Lk)("p",null," Intrinsic self-correction mechanisms in state-of-the-art LLMs were expected to enhance performance by refining responses based solely on the model's inherent capabilities. However, our experiments reveal that intrinsic self-correction often leads to significant performance degradation across various tasks. ",-1)),t[17]||(t[17]=(0,a.Lk)("h4",null,"Experimental Results",-1)),t[18]||(t[18]=(0,a.Lk)("p",null,"Below are the key experimental results demonstrating the failures of intrinsic self-correction:",-1)),(0,a.Lk)("div",x,[(0,a.bF)(U,{id:"boolq-table",data:n,style:{width:"85%"}},{default:(0,a.k6)((function(){return[(0,a.bF)(z,{prop:"model",label:"Model",align:"center"}),(0,a.bF)(z,{prop:"acc1",label:"ACC₁ (↓ΔACC) (%)",align:"center"}),(0,a.bF)(z,{prop:"overturned",label:"✓→✗ (%)",align:"center"},{default:(0,a.k6)((function(e){return[(0,a.eW)((0,i.v_)(e.row.overturned),1)]})),_:1})]})),_:1})]),t[19]||(t[19]=(0,a.Lk)("p",{class:"table-caption"},"Table 1: Self-correction performance on the Yes/No question answering task.",-1)),t[20]||(t[20]=(0,a.Lk)("div",{class:"observation-box"},[(0,a.Lk)("strong",null,"Observation 1:"),(0,a.eW)(" Self-correction can fail in diverse tasks. For SOTA LLMs, self-correction failures are reduced but not solved. They are even worse in certain tasks. ")],-1))])]),(0,a.Lk)("div",P,[t[32]||(t[32]=(0,a.Lk)("h3",null,[(0,a.Lk)("span",{class:"section-title"},"Interpretation")],-1)),(0,a.Lk)("div",S,[t[31]||(t[31]=(0,a.Lk)("p",null," We propose three interpretation methods to understand how and why intrinsic self-correction fails in different tasks: ",-1)),(0,a.Lk)("div",A,[(0,a.bF)(Q,{gutter:20},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:12},{default:(0,a.k6)((function(){return t[22]||(t[22]=[(0,a.Lk)("h4",null,"1. Internal Answer Wavering",-1),(0,a.Lk)("p",null," We analyze LLMs' internal token representations at each layer to track how confidence in different answers evolves. Our findings show that: ",-1),(0,a.Lk)("ul",null,[(0,a.Lk)("li",null,"Self-correction increases internal answer wavering from 8.3% to 14.1%"),(0,a.Lk)("li",null,'Prompting with "Are you sure?" produces nearly identical confidence patterns as directly stating "You are wrong"')],-1)])})),_:1}),(0,a.bF)(O,{span:12},{default:(0,a.k6)((function(){return t[23]||(t[23]=[(0,a.Lk)("img",{src:s,alt:"Internal Confidence Analysis",class:"method-image"},null,-1)])})),_:1})]})),_:1}),t[24]||(t[24]=(0,a.Lk)("div",{class:"observation-box"},[(0,a.Lk)("strong",null,"Observation 2:"),(0,a.eW)(" Self-correction causes internal answer wavering, which could further lead to wrong final answers. Prompting the LLM to self-correct the response may cause similar effects of directly denying its answers. ")],-1))]),(0,a.Lk)("div",M,[(0,a.bF)(Q,{gutter:20},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:11},{default:(0,a.k6)((function(){return t[25]||(t[25]=[(0,a.Lk)("h4",null,"2. Token Attribution Analysis: Prompt Bias",-1),(0,a.Lk)("p",null," Using our PACT (Prompt Attribution and Contribution Tracking) method, we measure how different parts of the input influence the model's decisions: ",-1),(0,a.Lk)("ul",null,[(0,a.Lk)("li",null,"When correct answers are overturned, models show stronger attribution to refinement prompts"),(0,a.Lk)("li",null,"When correct answers are retained, models maintain focus on the original question")],-1)])})),_:1}),(0,a.bF)(O,{span:1}),(0,a.bF)(O,{span:11},{default:(0,a.k6)((function(){return t[26]||(t[26]=[(0,a.Lk)("img",{src:l,alt:"Token Attribution Analysis",class:"method-image"},null,-1)])})),_:1})]})),_:1}),t[27]||(t[27]=(0,a.Lk)("div",{class:"observation-box"},[(0,a.Lk)("strong",null,"Observation 3:"),(0,a.eW)(" Self-correction fails since LLMs are biased towards the refinement prompt rather than the original question. ")],-1))]),(0,a.Lk)("div",W,[(0,a.bF)(Q,{gutter:20},{default:(0,a.k6)((function(){return[(0,a.bF)(O,{span:11},{default:(0,a.k6)((function(){return t[28]||(t[28]=[(0,a.Lk)("h4",null,"3. Human-like Cognitive Bias Analysis",-1),(0,a.Lk)("p",null," In complex tasks, we identify three types of human-like cognitive biases that emerge during self-correction: ",-1),(0,a.Lk)("ul",null,[(0,a.Lk)("li",null,[(0,a.Lk)("strong",null,"Overthinking:"),(0,a.eW)(' Excessive reasoning without taking correct actions (avg. 15.4 vs 5.3 "think" steps)')]),(0,a.Lk)("li",null,[(0,a.Lk)("strong",null,"Cognitive Overload:"),(0,a.eW)(" Forgetting critical information when processing long prompts")]),(0,a.Lk)("li",null,[(0,a.Lk)("strong",null,"Perfectionism Bias:"),(0,a.eW)(" Over-optimization leading to constraint violations")])],-1)])})),_:1}),(0,a.bF)(O,{span:13},{default:(0,a.k6)((function(){return t[29]||(t[29]=[(0,a.Lk)("img",{src:c,alt:"Human Cognitive Biases",class:"method-image"},null,-1)])})),_:1})]})),_:1}),t[30]||(t[30]=(0,a.Lk)("div",{class:"observation-box"},[(0,a.Lk)("strong",null,"Observation 4:"),(0,a.eW)(' In complex tasks, LLMs exhibit human-like cognitive biases during self-correction: (1) Overthinking: LLM performs excessive "think" without taking correct actions; (2) Cognitive overload: LLM forgets the correct command syntax when processing long prompt; (3) Perfectionism bias: LLM wants to be more efficient, but instead violates environmental restrictions. ')],-1))])])]),(0,a.Lk)("div",G,[t[41]||(t[41]=(0,a.Lk)("h3",null,[(0,a.Lk)("span",{class:"section-title"},"Alleviation")],-1)),(0,a.Lk)("div",I,[t[33]||(t[33]=(0,a.Lk)("p",null," Based on our findings that self-correction failures are mainly due to model's behavior of changing answers when meeting refinement prompts, we propose two simple yet effective strategies: ",-1)),t[34]||(t[34]=(0,a.Lk)("div",{style:{"text-align":"center"}},[(0,a.Lk)("img",{src:u,alt:"Question Repeating",class:"responsive-image",style:{width:"80%"}})],-1)),t[35]||(t[35]=(0,a.Lk)("div",{class:"solution-item"},[(0,a.Lk)("h4",null,"1. Question Repeating"),(0,a.Lk)("p",null,' We attach the original question to the end of the refinement prompt to reduce recency bias. For example: "Are you sure? Think and answer again." → "Are you sure? Think and answer again. Is human a kind of animals?" ')],-1)),t[36]||(t[36]=(0,a.Lk)("div",{class:"solution-item"},[(0,a.Lk)("h4",null,"2. Low-cost Supervised Fine-Tuning (SFT)"),(0,a.Lk)("p",null," We fine-tune models with extremely few samples (4 for Llama, 10 for GPT) selected from ✓→✗ cases, without introducing external knowledge. The cost is only $0.004 and 3 minutes. ")],-1)),t[37]||(t[37]=(0,a.Lk)("h4",null,"Key Results",-1)),t[38]||(t[38]=(0,a.Lk)("ul",null,[(0,a.Lk)("li",null,"Both strategies significantly reduce self-correction failures in Yes/No questions"),(0,a.Lk)("li",null,"SFT almost eliminates all ✓→✗ cases"),(0,a.Lk)("li",null,"Models fine-tuned on Yes/No questions can generalize to complex tasks")],-1)),(0,a.Lk)("div",B,[(0,a.bF)(U,{id:"mitigate-table",data:q,style:{width:"85%"},"row-class-name":function(e){return e.isGroupEnd?"border-bottom":""}},{default:(0,a.k6)((function(){return[(0,a.bF)(z,{prop:"model",label:"Model",align:"center"},{default:(0,a.k6)((function(e){return[(0,a.Lk)("span",{class:(0,i.C4)({"bold-text":e.row.model.includes("SFT")})},(0,i.v_)(e.row.model),3)]})),_:1}),(0,a.bF)(z,{label:"ACC₁ (↓ΔACC) (%)",align:"center"},{default:(0,a.k6)((function(e){return["object"===(0,o.A)(e.row.acc1)?((0,a.uX)(),(0,a.CE)("span",{key:0,class:(0,i.C4)({"bold-text":e.row.acc1.bold})},(0,i.v_)(e.row.acc1.value)+" ("+(0,i.v_)(e.row.acc1.delta)+") ",3)):((0,a.uX)(),(0,a.CE)(a.FK,{key:1},[(0,a.eW)((0,i.v_)(e.row.acc1),1)],64))]})),_:1}),(0,a.bF)(z,{label:"✓→✗ (%)",align:"center"},{default:(0,a.k6)((function(e){return["object"===(0,o.A)(e.row.overturned)?((0,a.uX)(),(0,a.CE)("span",{key:0,class:(0,i.C4)({"bold-text":e.row.overturned.bold})},(0,i.v_)(e.row.overturned.value),3)):((0,a.uX)(),(0,a.CE)(a.FK,{key:1},[(0,a.eW)((0,i.v_)(e.row.overturned),1)],64))]})),_:1})]})),_:1},8,["row-class-name"])]),t[39]||(t[39]=(0,a.Lk)("p",{class:"table-caption table-caption-with-spacing"},"Table 2: Alleviating self-correction failure on Yes/No question answering task using question repeating and supervised fine-tuning (SFT), where question repeating reduces ✓→✗ (%) and SFT almost eliminates all correct→wrong cases.",-1)),(0,a.Lk)("div",E,[(0,a.bF)(U,{id:"generalize-table",data:R,style:{width:"85%"},"row-class-name":function(e){return e.isGroupEnd?"border-bottom":""}},{default:(0,a.k6)((function(){return[(0,a.bF)(z,{prop:"task",label:"Task",align:"center"},{default:(0,a.k6)((function(e){return[(0,a.Lk)("span",null,(0,i.v_)(e.row.task),1)]})),_:1}),(0,a.bF)(z,{label:"Model",align:"center"},{default:(0,a.k6)((function(e){return[(0,a.Lk)("span",{class:(0,i.C4)({"bold-text":e.row.model.includes("SFT")})},(0,i.v_)(e.row.model),3)]})),_:1}),(0,a.bF)(z,{label:"ACC₁ (↓ΔACC) (%)",align:"center"},{default:(0,a.k6)((function(e){return["object"===(0,o.A)(e.row.acc1)?((0,a.uX)(),(0,a.CE)("span",{key:0,class:(0,i.C4)({"bold-text":e.row.acc1.bold})},(0,i.v_)(e.row.acc1.value)+" ("+(0,i.v_)(e.row.acc1.delta)+") ",3)):((0,a.uX)(),(0,a.CE)(a.FK,{key:1},[(0,a.eW)((0,i.v_)(e.row.acc1),1)],64))]})),_:1}),(0,a.bF)(z,{label:"✓→✗ (%)",align:"center"},{default:(0,a.k6)((function(e){return["object"===(0,o.A)(e.row.overturned)?((0,a.uX)(),(0,a.CE)("span",{key:0,class:(0,i.C4)({"bold-text":e.row.overturned.bold})},(0,i.v_)(e.row.overturned.value),3)):((0,a.uX)(),(0,a.CE)(a.FK,{key:1},[(0,a.eW)((0,i.v_)(e.row.overturned),1)],64))]})),_:1})]})),_:1},8,["row-class-name"])]),t[40]||(t[40]=(0,a.Lk)("p",{class:"table-caption"},"Table 3: LLMs fine-tuned on Yes/No question answering task can generalize to complex tasks, where ACC is increased and ✓→✗ (%) is decreased across decision making, reasoning and programming tasks.",-1))])]),t[42]||(t[42]=(0,a.Lk)("div",{class:"section",id:"resources"},[(0,a.Lk)("h3",null,[(0,a.Lk)("span",{class:"section-title"},"Resources")]),(0,a.Lk)("div",{class:"section-content"},[(0,a.Lk)("p",null," Access our code repository through the following links: "),(0,a.Lk)("ul",null,[(0,a.Lk)("li",null,[(0,a.Lk)("a",{href:"https://anonymous.4open.science/r/SC-15FB/",target:"_blank"},"Project Code")])])])],-1))]})),_:1})]})),_:1})])])}}});var R=t(71241);const X=(0,R.A)(q,[["__scopeId","data-v-04cd88ca"]]),j=X},65423:(e,n,t)=>{e.exports=t.p+"99e3ab032bc4c2c8.mov"},61021:(e,n,t)=>{e.exports=t.p+"ea063ee27ac4790c.mov"},24998:(e,n,t)=>{e.exports=t.p+"c213bc5ff1150125.mov"},42646:(e,n,t)=>{e.exports=t.p+"58d93f7d025a2ecf.mov"},5745:(e,n,t)=>{var o=t(46518),a=t(77240),i=t(23061);o({target:"String",proto:!0,forced:i("bold")},{bold:function(){return a(this,"b","","")}})}}]);
//# sourceMappingURL=760.670a34d1.js.map